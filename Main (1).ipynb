{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# ==================== SPLIT FUNCTIONALITY ====================\n",
    "\n",
    "def split(source_dir):\n",
    "    \"\"\"\n",
    "    Processes Excel files from source_dir by scanning its subdirectories,\n",
    "    applying specific conditions, and copying the files into two new folders:\n",
    "      - NewFormat\n",
    "      - OldFormat\n",
    "    These folders are created within a \"Consolidated_Data\" folder located in the\n",
    "    parent directory of source_dir.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (new_format_dir, old_format_dir)\n",
    "    \"\"\"\n",
    "    parent_dir = os.path.dirname(source_dir)\n",
    "    consolidated_dir = os.path.join(parent_dir, 'Consolidated_Data')\n",
    "    os.makedirs(consolidated_dir, exist_ok=True)\n",
    "    \n",
    "    new_format_dir = os.path.join(consolidated_dir, 'NewFormat')\n",
    "    old_format_dir = os.path.join(consolidated_dir, 'OldFormat')\n",
    "    os.makedirs(new_format_dir, exist_ok=True)\n",
    "    os.makedirs(old_format_dir, exist_ok=True)\n",
    "    \n",
    "    folders_without_copy = []\n",
    "    excel_extensions = ('.xls', '.xlsx')\n",
    "    \n",
    "    def get_excel_sheet_count(file_path):\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, sheet_name=None)\n",
    "            return len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def check_f16_ends_with_00(file_path):\n",
    "        try:\n",
    "            if file_path.lower().endswith('.xls'):\n",
    "                workbook = xlrd.open_workbook(file_path)\n",
    "                sheet = workbook.sheet_by_index(0)\n",
    "                cell_value = sheet.cell_value(15, 5)\n",
    "                if isinstance(cell_value, str) and cell_value.endswith(':00'):\n",
    "                    return True\n",
    "                elif isinstance(cell_value, (int, float)) and str(int(cell_value)).endswith(':00'):\n",
    "                    return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading F16 from {file_path}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def contains_transaction_twice(file_name):\n",
    "        return file_name.lower().count('trans') >= 2\n",
    "    \n",
    "    def get_unique_file_path(destination_folder, file_name):\n",
    "        base_name, extension = os.path.splitext(file_name)\n",
    "        counter = 1\n",
    "        new_file_name = file_name\n",
    "        while os.path.exists(os.path.join(destination_folder, new_file_name)):\n",
    "            new_file_name = f\"{base_name}_{counter}{extension}\"\n",
    "            counter += 1\n",
    "        return new_file_name\n",
    "    \n",
    "    def contains_criteria(file_name):\n",
    "        criteria_variations = ['criteria', 'citeria', 'criertia', 'criteeria', \n",
    "                                 'critiera', 'criteera', 'creiteira', 'critieira', 'criterita']\n",
    "        return any(variation in file_name.lower() for variation in criteria_variations)\n",
    "    \n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if any(keyword in os.path.basename(root).lower() for keyword in ['daily', 'payroll']):\n",
    "            print(f\"Skipping directory: {root}\")\n",
    "            continue\n",
    "        \n",
    "        excel_files_in_dir = [file for file in files if file.lower().endswith(excel_extensions)]\n",
    "        if excel_files_in_dir:\n",
    "            subdirectory_full_path = os.path.abspath(root)\n",
    "            conditions_met = 0\n",
    "            for file in excel_files_in_dir:\n",
    "                file_path = os.path.join(root, file)\n",
    "                sheet_count = get_excel_sheet_count(file_path)\n",
    "                if check_f16_ends_with_00(file_path):\n",
    "                    if contains_criteria(file.lower()) and sheet_count == 2:\n",
    "                        dest_file_path = os.path.join(new_format_dir, get_unique_file_path(new_format_dir, file))\n",
    "                        shutil.copy2(file_path, dest_file_path)\n",
    "                        print(f\"Moved {file} to NewFormat\")\n",
    "                        conditions_met += 1\n",
    "                    elif contains_criteria(file.lower()) and sheet_count > 2:\n",
    "                        dest_file_path = os.path.join(old_format_dir, get_unique_file_path(old_format_dir, file))\n",
    "                        shutil.copy2(file_path, dest_file_path)\n",
    "                        print(f\"Moved {file} to OldFormat\")\n",
    "                        conditions_met += 1\n",
    "                    elif ('revenue' in file.lower() and \n",
    "                          not any(keyword in file.lower() for keyword in ['all', 'lot', 'park', 'exception']) and \n",
    "                          sheet_count > 2 and not contains_transaction_twice(file.lower())):\n",
    "                        dest_file_path = os.path.join(old_format_dir, get_unique_file_path(old_format_dir, file))\n",
    "                        shutil.copy2(file_path, dest_file_path)\n",
    "                        print(f\"Moved {file} to OldFormat\")\n",
    "                        conditions_met += 1\n",
    "                    elif ('revenue' in file.lower() and \n",
    "                          not any(keyword in file.lower() for keyword in ['all', 'lot', 'park', 'exception']) and \n",
    "                          sheet_count == 2 and not contains_transaction_twice(file.lower())):\n",
    "                        dest_file_path = os.path.join(new_format_dir, get_unique_file_path(new_format_dir, file))\n",
    "                        shutil.copy2(file_path, dest_file_path)\n",
    "                        print(f\"Moved {file} to NewFormat\")\n",
    "                        conditions_met += 1\n",
    "            if conditions_met == 0 and 'revis' not in os.path.basename(root).lower() \\\n",
    "               and os.path.basename(root) != os.path.basename(os.path.dirname(root)):\n",
    "                folders_without_copy.append(subdirectory_full_path)\n",
    "    \n",
    "    if folders_without_copy:\n",
    "        print(\"\\nFolders with Excel files that did not meet any conditions:\")\n",
    "        for folder in folders_without_copy:\n",
    "            print(f\"- {folder}\")\n",
    "    else:\n",
    "        print(\"\\nAll directories with Excel files met at least one condition.\")\n",
    "    \n",
    "    return new_format_dir, old_format_dir\n",
    "\n",
    "# ==================== DEDUPLICATION FUNCTIONALITY ====================\n",
    "\n",
    "def read_excel_file_for_dedup(file_path):\n",
    "    workbook = xlrd.open_workbook(file_path)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    f16_value = sheet.cell_value(15, 5)\n",
    "    j7_value = sheet.cell_value(6, 9)\n",
    "    f16_date_part = str(f16_value)[:8]\n",
    "    return f16_date_part, j7_value\n",
    "\n",
    "def deduplicate(source_dir):\n",
    "    print(f\"\\nProcessing folder for deduplication: {source_dir}\")\n",
    "    f16_dict = {}\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        file_path = os.path.join(source_dir, file_name)\n",
    "        try:\n",
    "            f16_date_part, j7_value = read_excel_file_for_dedup(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue\n",
    "        if f16_date_part not in f16_dict:\n",
    "            f16_dict[f16_date_part] = []\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        f16_dict[f16_date_part].append({\n",
    "            'file_name': file_name,\n",
    "            'file_size': file_size,\n",
    "            'j7_value': j7_value,\n",
    "            'file_path': file_path\n",
    "        })\n",
    "    \n",
    "    for f16_date_part, files in f16_dict.items():\n",
    "        if len(files) > 1:\n",
    "            print(f\"\\nShared F16 (first 8 characters) content: {f16_date_part}\")\n",
    "            seen_files = set()\n",
    "            for i in range(len(files)):\n",
    "                for j in range(i + 1, len(files)):\n",
    "                    file1 = files[i]\n",
    "                    file2 = files[j]\n",
    "                    if file1['file_size'] == file2['file_size']:\n",
    "                        if file1['file_path'] not in seen_files and file2['file_path'] not in seen_files:\n",
    "                            print(f\"\\nFlagged duplicate files with F16: {f16_date_part}\")\n",
    "                            print(f\"File 1: {file1['file_name']} (Size: {file1['file_size']} bytes)\")\n",
    "                            print(f\"File 2: {file2['file_name']} (Size: {file2['file_size']} bytes)\")\n",
    "                            try:\n",
    "                                os.remove(file2['file_path'])\n",
    "                                print(f\"Deleted duplicate file: {file2['file_path']}\")\n",
    "                                seen_files.add(file2['file_path'])\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error deleting file {file2['file_path']}: {e}\")\n",
    "\n",
    "# ==================== MERGE (CONSOLIDATION) FUNCTIONALITY ====================\n",
    "\n",
    "def findHeader(df):\n",
    "    row = df.index[df.iloc[:, 0] == \"Entry Time\"]\n",
    "    return row[0] + 1\n",
    "\n",
    "def findFooter(df):\n",
    "    row = df.index[df.iloc[:, 0].str.contains(\"Totals\", case=False, na=False)]\n",
    "    totalrows = len(df)\n",
    "    numFooterRows = totalrows - row[0]\n",
    "    return numFooterRows\n",
    "\n",
    "def findStation(df):\n",
    "    row = df.index[df.iloc[:, 0].str.contains(\"Station\", case=False, na=False)]\n",
    "    if row.empty:\n",
    "        return False\n",
    "    else:\n",
    "        return row[0]\n",
    "\n",
    "def processSheetwithHeader(eFile, sName, oFile, adjust_footer=False):\n",
    "    print(\"Processing with header:\", eFile)\n",
    "    df = pd.read_excel(eFile, sheet_name=sName)\n",
    "    rowHeader = findHeader(df)\n",
    "    rowFooter = findFooter(df)\n",
    "    StationRow = findStation(df)\n",
    "    if StationRow is not False:\n",
    "        station = df.iloc[StationRow, 0]\n",
    "        start_index = station.find(\"Station: \") + len(\"Station: \")\n",
    "        end_index = station.find(\"-\", start_index)\n",
    "        Station = station[start_index:end_index]\n",
    "    df = pd.read_excel(eFile, sheet_name=sName, skiprows=rowHeader, skipfooter=rowFooter+1)\n",
    "    if StationRow is not False:\n",
    "        df.insert(df.columns.get_loc(\"Prev Stn\") + 1, 'Station', Station)\n",
    "    df.rename(columns={'Trans Time': 'Transaction Time',\n",
    "                       'Media\\n': 'Media',\n",
    "                       'Media ID\\n': 'Media',\n",
    "                       'Prev Stn': 'Previous Station',\n",
    "                       'Trans Type': 'Transaction Type'}, inplace=True)\n",
    "    df = df[['Entry Time', 'Transaction Time', 'Previous Station', 'Station', 'Media', 'Transaction Type', 'Revenue']]\n",
    "    df.to_csv(oFile, mode=\"a\", index=False, header=True)\n",
    "\n",
    "def processSheetnoHeader(eFile, sName, oFile, adjust_footer=False):\n",
    "    print(\"Processing without header:\", eFile)\n",
    "    df = pd.read_excel(eFile, sheet_name=sName)\n",
    "    rowHeader = findHeader(df)\n",
    "    rowFooter = findFooter(df)\n",
    "    if adjust_footer:\n",
    "        rowFooter = rowFooter + 1\n",
    "    StationRow = findStation(df)\n",
    "    if StationRow is not False:\n",
    "        station = df.iloc[StationRow, 0]\n",
    "        start_index = station.find(\"Station: \") + len(\"Station: \")\n",
    "        end_index = station.find(\"-\", start_index)\n",
    "        Station = station[start_index:end_index]\n",
    "    df = pd.read_excel(eFile, sheet_name=sName, skiprows=rowHeader, skipfooter=rowFooter)\n",
    "    if StationRow is not False:\n",
    "        df.insert(df.columns.get_loc(\"Prev Stn\") + 1, 'Station', Station)\n",
    "    df.rename(columns={'Trans Time': 'Transaction Time',\n",
    "                       'Media\\n': 'Media',\n",
    "                       'Media ID\\n': 'Media',\n",
    "                       'Prev Stn': 'Previous Station',\n",
    "                       'Trans Type': 'Transaction Type'}, inplace=True)\n",
    "    df = df[['Entry Time', 'Transaction Time', 'Previous Station', 'Station', 'Media', 'Transaction Type', 'Revenue']]\n",
    "    df.to_csv(oFile, mode=\"a\", index=False, header=False)\n",
    "\n",
    "def consolidateMultipleSheetswithHeadersandFooters(fileList, folder_path, oFile):\n",
    "    firstFile = fileList[0]\n",
    "    for eFile in fileList:\n",
    "        neweFile = os.path.join(folder_path, eFile)\n",
    "        print(\"Processing File:\" + str(neweFile))\n",
    "        dfChecks = pd.ExcelFile(neweFile)\n",
    "        numSheets = len(dfChecks.sheet_names) + 1\n",
    "        adjust_footer = True if numSheets == 3 else False\n",
    "        if eFile == firstFile:\n",
    "            for i in range(2, numSheets):\n",
    "                cSheet = \"Sheet\" + str(i)\n",
    "                if i == 2:\n",
    "                    processSheetwithHeader(neweFile, cSheet, oFile, adjust_footer)\n",
    "                else:\n",
    "                    processSheetnoHeader(neweFile, cSheet, oFile, adjust_footer)\n",
    "        else:\n",
    "            for i in range(2, numSheets):\n",
    "                cSheet = \"Sheet\" + str(i)\n",
    "                processSheetnoHeader(neweFile, cSheet, oFile, adjust_footer)\n",
    "\n",
    "def consolidateFolder(folder_path):\n",
    "    fileList = [file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))]\n",
    "    output_file = os.path.join(folder_path, \"consolidated_data.csv\")\n",
    "    consolidateMultipleSheetswithHeadersandFooters(fileList, folder_path, output_file)\n",
    "    return output_file\n",
    "\n",
    "def merge(new_format_dir, old_format_dir):\n",
    "    \"\"\"\n",
    "    Consolidates the Excel files in each of the two folders into a single CSV file.\n",
    "    The consolidated file is saved as \"consolidated_data.csv\" within each folder.\n",
    "    \"\"\"\n",
    "    print(f\"\\nConsolidating folder: {new_format_dir}\")\n",
    "    output_file_new = consolidateFolder(new_format_dir)\n",
    "    print(f\"Consolidated data for NewFormat created at: {output_file_new}\")\n",
    "    \n",
    "    print(f\"\\nConsolidating folder: {old_format_dir}\")\n",
    "    output_file_old = consolidateFolder(old_format_dir)\n",
    "    print(f\"Consolidated data for OldFormat created at: {output_file_old}\")\n",
    "\n",
    "# ==================== CHOP FUNCTIONALITY ====================\n",
    "\n",
    "def chop(input_file, output_dir, suffix, max_size_mb=24):\n",
    "    \"\"\"\n",
    "    Splits a large CSV file into multiple smaller CSV files based on file size.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the large CSV file.\n",
    "        output_dir (str): Directory where the split files will be saved.\n",
    "        suffix (str): Suffix to append to each split file's name (e.g., \"new\" or \"old\").\n",
    "        max_size_mb (int, optional): Maximum file size in megabytes for each split file. Defaults to 24.\n",
    "    \"\"\"\n",
    "    max_size_bytes = max_size_mb * 1024 * 1024\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        header = infile.readline()\n",
    "        header_bytes = header.encode('utf-8')\n",
    "        file_count = 1\n",
    "        current_file_path = os.path.join(output_dir, f\"split_{file_count}_{suffix}.csv\")\n",
    "        outfile = open(current_file_path, 'w', encoding='utf-8')\n",
    "        outfile.write(header)\n",
    "        current_size = len(header_bytes)\n",
    "        for line in infile:\n",
    "            line_bytes = line.encode('utf-8')\n",
    "            if current_size + len(line_bytes) > max_size_bytes:\n",
    "                outfile.close()\n",
    "                file_count += 1\n",
    "                current_file_path = os.path.join(output_dir, f\"split_{file_count}_{suffix}.csv\")\n",
    "                outfile = open(current_file_path, 'w', encoding='utf-8')\n",
    "                outfile.write(header)\n",
    "                current_size = len(header_bytes)\n",
    "            outfile.write(line)\n",
    "            current_size += len(line_bytes)\n",
    "        outfile.close()\n",
    "\n",
    "def chop_all(new_format_dir, old_format_dir, max_size_mb=24):\n",
    "    \"\"\"\n",
    "    Applies the chop function to the consolidated_data.csv file in both NewFormat and OldFormat folders.\n",
    "    The split files are stored in a single folder called \"csv_chunks\" at the same level as NewFormat and OldFormat.\n",
    "    \n",
    "    The output files will be named like \"split_1_new.csv\", \"split_2_new.csv\" (for NewFormat) and \n",
    "    \"split_1_old.csv\", \"split_2_old.csv\" (for OldFormat).\n",
    "    \n",
    "    Parameters:\n",
    "        new_format_dir (str): Path to the NewFormat folder.\n",
    "        old_format_dir (str): Path to the OldFormat folder.\n",
    "        max_size_mb (int, optional): Maximum file size in MB for each split file.\n",
    "    \"\"\"\n",
    "    parent_folder = os.path.dirname(new_format_dir)\n",
    "    consolidated_new_file = os.path.join(new_format_dir, \"consolidated_data.csv\")\n",
    "    consolidated_old_file = os.path.join(old_format_dir, \"consolidated_data.csv\")\n",
    "    output_dir = os.path.join(parent_folder, \"csv_chunks\")\n",
    "    \n",
    "    print(f\"Chopping consolidated file in NewFormat: {consolidated_new_file}\")\n",
    "    chop(consolidated_new_file, output_dir, suffix=\"new\", max_size_mb=max_size_mb)\n",
    "    print(f\"Split files for NewFormat created in: {output_dir}\\n\")\n",
    "    \n",
    "    print(f\"Chopping consolidated file in OldFormat: {consolidated_old_file}\")\n",
    "    chop(consolidated_old_file, output_dir, suffix=\"old\", max_size_mb=max_size_mb)\n",
    "    print(f\"Split files for OldFormat created in: {output_dir}\")\n",
    "\n",
    "# ==================== MASTER WORKFLOW ====================\n",
    "\n",
    "def main(source_dir, max_chop_mb=24):\n",
    "    \"\"\"\n",
    "    Master function that:\n",
    "      1. Calls split() on the user-provided source directory.\n",
    "      2. Retrieves the generated NewFormat and OldFormat directories.\n",
    "      3. Runs deduplication on each of these directories.\n",
    "      4. Runs merge() on each of these directories to consolidate Excel files into CSV files.\n",
    "      5. Splits (chops) each consolidated CSV file into smaller chunks.\n",
    "    \n",
    "    The user needs only supply the source directory.\n",
    "    \"\"\"\n",
    "    print(f\"Starting split process for data in: {source_dir}\")\n",
    "    new_format_dir, old_format_dir = split(source_dir)\n",
    "    print(\"\\nSplit process complete.\")\n",
    "    \n",
    "    print(\"\\nStarting deduplication on NewFormat files...\")\n",
    "    deduplicate(new_format_dir)\n",
    "    print(\"\\nStarting deduplication on OldFormat files...\")\n",
    "    deduplicate(old_format_dir)\n",
    "    \n",
    "    print(\"\\nStarting merge (consolidation) process...\")\n",
    "    merge(new_format_dir, old_format_dir)\n",
    "    print(\"\\nMerge process complete.\")\n",
    "    \n",
    "    print(\"\\nStarting chop process on consolidated CSV files...\")\n",
    "    chop_all(new_format_dir, old_format_dir, max_size_mb=max_chop_mb)\n",
    "    \n",
    "    print(\"\\nAll processing complete.\")\n",
    "\n",
    "# ==================== Example Usage ====================\n",
    "# In a Jupyter Notebook, the user would simply call:\n",
    "# main(r'C:\\Users\\mvalsania25\\Desktop\\Parking_Complete', max_chop_mb=24)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
