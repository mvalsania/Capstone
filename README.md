# Ontario International Airport Dynamic Pricing Capstone

## Overview

This repository contains the data‐processing pipeline developed for my Data Science capstone project, which explores dynamic pricing strategies for parking lot facilities at Ontario International Airport (OIA). The pipeline:

1. **Splits** raw Excel data into standardized folders (`NewFormat`, `OldFormat`).
2. **De‐duplicates** files based on timestamp and size criteria.
3. **Merges** multiple sheets into consolidated CSVs.
4. **Chunks** large CSVs into manageable pieces for downstream analysis.

## Project Structure

```text
├── main.ipynb             # Jupyter notebook with ETL functions & master workflow
├── requirements.txt       # Python dependencies
├── Consolidated_Data/     # Generated folders from split()
│   ├── NewFormat/         # Files matching "new" schema
│   └── OldFormat/         # Files matching "old" schema
├── csv_chunks/            # Split CSV outputs by size
└── capstone.Rmd           # R Markdown for data exploration & visualization
```

## Installation

1. Clone this repository:

   ```bash
   git clone https://github.com/yourusername/oia-dynamic-pricing.git
   cd oia-dynamic-pricing
   ```

2. Create and activate a virtual environment (optional but recommended):

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # macOS/Linux
   venv\Scripts\activate   # Windows
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

## Usage

### 1. Run the full pipeline

```bash
python pipeline.py /path/to/raw_excel_data --max_chop_mb 24
```

* `/path/to/raw_excel_data`: Directory containing airport parking Excel files (can include nested subfolders).
* `--max_chop_mb`: Maximum size (in MB) for each CSV chunk (default: 24).

This will execute the following steps in sequence:

1. **split()**: Organize files into `Consolidated_Data/NewFormat` and `Consolidated_Data/OldFormat`.
2. **deduplicate()**: Remove duplicate spreadsheets based on cell F16 timestamp and file size.
3. **merge()**: Consolidate all sheets into `consolidated_data.csv` within each format folder.
4. **chop\_all()**: Split CSVs into `csv_chunks/`.

### 2. Import functions in Jupyter

In `notebooks/analysis.ipynb`, you can import and call individual functions:

```python
from pipeline import split, deduplicate, merge, chop_all, main

# Example:
new_dir, old_dir = split('data/raw')
deduplicate(new_dir)
merge(new_dir, old_dir)
chop_all(new_dir, old_dir)
```

## Function Reference

* `split(source_dir) → (new_format_dir, old_format_dir)`

  * Scans `source_dir`, applies naming and sheet‐count rules, copies files into `NewFormat` or `OldFormat`.
* `deduplicate(source_dir)`

  * Reads cell F16 (timestamp) and J7, groups files by F16 prefix, removes size‐identical duplicates.
* `merge(new_format_dir, old_format_dir)`

  * Reads each Excel sheet, extracts transaction records, and writes to `consolidated_data.csv`.
* `chop(input_file, output_dir, suffix, max_size_mb)`

  * Splits a large CSV into size‐bounded chunks, preserving header row.
* `chop_all(new_format_dir, old_format_dir, max_size_mb)`

  * Applies `chop()` to both `NewFormat` and `OldFormat` consolidated CSVs.
* `main(source_dir, max_chop_mb)`

  * Wrapper that runs all steps in order.

## Data Source

* Consolidated and split CSV files (`csv_chunks/`) generated by the pipeline (`pipeline.py`).
* Run `main(source_dir)` to process raw Excel exports into the `Consolidated_Data/` and `csv_chunks/` folders, which serve as inputs for analysis.

## 4. Data Exploration & Visualization (R Markdown)

The `notebooks/Capstone.Rmd` file (rendered to HTML) performs in‐depth analysis and visualization on the processed parking data:

1. **Data Ingestion**

   * Reads all CSV chunks from `csv_chunks/`.
   * Combines them into a single DataFrame and writes `combined_data.csv`.
2. **Cleaning & Feature Engineering**

   * Renames columns (`Entry`, `Exit`, `Type`, `Revenue`, `Media`).
   * Filters records, handles “R” payments by merging payment and exit entries.
   * Converts `Entry` and `Exit` to POSIXct (`America/Los_Angeles`).
   * Computes `Length_of_Stay` (minutes) and extracts `Year`, `Month`.
   * Derives `Lot` from station codes (e.g., “A43” → `43`).
3. **Primary Metrics & Outputs**

   * **Filtered Data**: saved as `filtered_data.csv`.
   * **Yearly Counts**: volume of transactions per year.
   * **Revenue Summaries**: total revenue by Lot over time (monthly), plotted as line charts.
   * **Duration Bins**: revenue and count breakdowns by `Length_of_Stay` bins (e.g., 0–30 min, 30–60 min, 1–12 hrs, etc.), with stacked bar charts.
   * **Average Daily Rate**: for multi‐day stays (> 24 hrs), excluding Lot 6, plotted over time.
   * **Occupancy Trends**: weekly and daily occupancy counts per Lot (excluding Lot 6) over multiple years.
   * **Revenue Distribution**: boxplots of revenue per hour for 1–4 hr stays (Lot 2, 2024).
   * **Frequency Analysis**: histograms and frequency tables for specific Lot/year revenue values.
   * **Short‐Stay Patterns**: histogram distributions of stays under 30 min, colored by zero vs. non‐zero revenue.

Run and render this R Markdown via RStudio or:

```bash
Rscript -e "rmarkdown::render('notebooks/Capstone.Rmd')"
```

---

Unfortunately, the full report cannot be shared as it contains confidential data.
